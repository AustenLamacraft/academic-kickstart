---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Everything Wrong With the Cambridge Exam System"
subtitle: ""
summary: ""
authors: []
tags: []
categories: []
date: 2021-06-01T12:01:50+01:00
lastmod: 2021-06-01T12:01:50+01:00
featured: false
draft: true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

I've just discharged my duties as examiner for Part II Physics, the third year of the Cambridge Physics course[^3].

[^3]: Nothing has the right name in Cambridge.

When I wasn't fielding questions from candidates, I've had some time to think about the shortcomings of the process. It's a process I'm part of, so any criticisms that follow are also self-criticisms (you'll see I'm following the Dominic Cummings approach here). 

{{% toc %}}

## How it works

For those familiar with other (non-UK) systems, here's a short summary of how exams work at Cambridge, which I believe is fairly typical of the UK as a whole.

0. The syllabus is set by the lecturer but requires approval from a teaching committee long before the start of the year.
1. The lecturer prepares a course handout (i.e. a short book) and / or lecture slides, possibly inheriting some material from the previous lecturer. 
2. OK: *normally* inheriting all the material from the previous lecturer, including the problem sets, and the mistakes in the problem sets. Who has time to write a book?
3. The department appoints examiners, who will produce the exams for each course in a year. 
4. The senior examiner decides which examiner gets which course. Nobody wants Relativity.
5. Or Astrophysical Fluid Dynamics. Not even the astrophysicist.
6. The examiners draft papers.
7. They exchange papers and check each others, to some degree.
8. They show them to the lecturer, who tells them their notation is wrong, and that topic isn't on the course. Also they added some stuff this year.
9. Eventually they show them to (two) **external examiners**, academics from other institutions. At each of these stages (mostly) good feedback is obtained, changes are made, errors are removed (and introduced). Much time is spent speculating *what* students will do when faced with a question.
10. The students sit the exams. There are errors, big and small. Papers are marked ("graded"); appeals are heard.
11. The examiners produce a **class list**, in which candidate is assigned a class. More about this below. 

In other words, we pretend that we are an exam board, the kind that deals with tens of thousands of candidates per year. The kind with a published syllabus, practice papers, and the like[^4]. The reality is a bit different, as we'll see.

[^4]: Funnily enough, the University does own [an exam board](https://en.wikipedia.org/wiki/Oxford,_Cambridge_and_RSA_Examinations). What use does it make of its expertise and tech in its own internal exams, do you think?

And for those who *aren't* familiar with other systems (of whom there are lots in Cambridge), here's how it works at a typical [US R1 University](https://en.wikipedia.org/wiki/Research_I_university). Since we'd like to regard many of these as our peer institutions, we should be curious.

0. The teaching committee assigns the lecturer.
1. The lecturer sets the syllabus.
2. The lecturer teaches the course, normally using a textbook
3. The lecturer grades problem sets, often with teaching assistants (TAs) for large courses.
4. The lecturer sets the (midterm exam and) final exam.
5. The lecturer grades the exams, sometimes with TAs.
6. The lecturer awards grades.
7. All of the collation of data to form GPAs, etc. is handled (automatically now) by the administration.

There are two points I want to make about these two systems

First, though it's a bit cheesy to point it out, the big difference between the UK and US systems is that the former is built on **collective responsibility and bureaucracy over a sprawling domain** while the latter relies on **individual responsibility over a much more narrowly defined domain**. In the UK system, I'm doing everything from writing exams to making a spreadsheet to determine your degree class from collated marks â€“ the equivalent of GPA. The latter is a purely administrative task should be fully automatic, and indeed has been for years in the US context. 

I've actually made the UK system sound much more well defined than it actually is. 

Second, you'll see that in the US system there is no separate examiner role, and no external examiner[^2]. There is no source of moderation *at all*. ~~As a result Harvard and Stanford have terrible reputations and nobody wants to go~~. This lack of moderation results in *zero reputational damage*.

[^2]: When I worked at the University of Virginia, the only exam set by a committee in the manner of the UK system was the graduate qualifying exam, a hurdle to be cleared prior to registration on the PhD program. I believe this is typical.

Reputation isn't everything, of course. We should be proud that we run (moderately-) moderated assessment. We might as well be, given the vast amounts of time spent on it, and the fact that nobody else cares. The moderation is probably good for the students, in the sense that the outcome is less "noisy", though as I'll argue, the system on which it is based is on the whole *bad* for students.

### Semi-professional teachers

In several ways the US model implicitly acknowledges that faculty are not professional teachers.


The promise of being taught by world-leading researchers is routinely dangled in the face of students seeking entry into our top universities. 

If these researchers were tasked with teaching you how to do research like them, that would be one thing (success not guaranteed).

### Excursion: how did exams change?

Previous tripos questions. 

Curse of knowledge.

Under the simplest assumption of a constant error rate, a question that's twice as long is twice as likely to have an error. Additionally ,, the increased length often takes the form of mathematical expressions provided to guide candidates to the answer, and these are more likely to contain errors.

Bottomless pit of scaffolding

A game of trying to get the students to do what you want, without explicitly telling them what to do.

The more scaffolding, the more room of biased outcomes and errors.

Scaling as a taboo 

Syllabus get changed..

## The stakes are incredibly low, and high

Just like our other Class system, the system of [degree classification](https://en.wikipedia.org/wiki/British_undergraduate_degree_classification) used in the UK is an anachronism.


It's a bit like reading about the Imperial system, isn't it?

Shannon information 

Both sides 

Wikipedia

Grade inflation is an emotive term, so let's just focus on the numbers. In 2002 60% of the candidates got firsts or 2.1s in Part II. In 2019 (the last normal year) it was 82%[^1]. 

[^1]: Note that we have a four year course culminating in Part III (natch), but entry to the fourth year is selective, requiring a 2.1, so a comparison isn't as meaningful. Note this system also has a massive downside: allowing a student to continue to a fourth year and then graduating them with a 2.2 would be awkward. Unless, of course, faculty were *confident* about the added value of a fourth year.

That Wikipedia article is wide of the mark when it talks about "student-demanded grade inflation". Inflation has been driven by faculty (of course: it's them giving out the degrees)

It's important to emphasize that I'm not trying to be mean, saying your 2019 low 2.1 is actually a 2002 2.2. The reason I'm not saying that is because the whole notion of a degree class is meaningless anyway. 

[Hardy on the tripos](https://mathshistory.st-andrews.ac.uk/Extras/Hardy_Tripos/)

> The examiner is not allowed to content himself with testing the competence and the knowledge of the candidates; his instructions are to provide a test of more than that, of initiative, imagination, and even of some sort of originality. And as there is only one test of originality in mathematics, namely the accomplishment of original work, and as it is useless to ask a youth of twenty-two to perform original research under examination conditions, the examination necessarily degenerates into a kind of game, and instruction for it into initiation into a series of stunts and tricks. 

> I should advise them to let down the standard at every opportunity; to give first classes to almost every candidate who applied; to crowd the syllabus with advanced subjects, until it was humanly impossible to show reasonable knowledge of them under the conditions of the examination. In this way, in the course of years, they might succeed in corrupting the value of the prizes which they have to offer, and in all probability time would do the rest.

### Apples and Oranges

At the heart of any classification scheme, whether the British honours system of the GPA used in North America and elsewhere, is the spurious idea of comparing student's attainment in totally different subjects. 

This idea runs all the way through the Cambridge Triposes. By the second year (Part IB) of Natural Sciences, students specializing in physical and biological Natural Sciences will generally pursue completely different courses, yet they all classed together. By the third year there's only one subject, so these are totally disjoint. 

This system has some amusing features. When I was senior examiner for Part IB, I received a mysterious envelope. My instructions were not to open this envelope until the final meeting of all the senior examiners for Part IB, and only if the class distribution for my subject differed by more than a certain amount from a pre-agreed distribution. 

What was inside the envelope? The performance of the physics students in Part IA *the previous year*. In other words, we could give the physics students better results on average than the chemists or the biologists if we could show that they were better students, according to their performance 12 months earlier.

Personally, I hate this idea of looking at previous performance. It seems antithetical both to the idea that we are adding value year on year, and that each year stands alone. Anyway, this procedure is useful for our discussion because it acknowledges one thing: to judge the relative difficulty of two exams, get the same people to sit them (under standardized conditions) and compare their performance. If you are willing to make the assumption that relative difficulty is transitive, you can judge three exams A, B, and C by getting one group to take exam A and C, and another to take B and C. *This* is why last year's results were inside that envelope. 


<!-- This is situation in Part II of the physics tripos, for example, where there are four core papers and some options.  -->

I can't think of any other way of doing things that is remotely scientific. The idea that examiners within a subject will correctly (in the sense of agreeing with the above measure) judge the relative difficulty of questions is absurd, and contradicted time and again by reality. The situation only gets worse as we look at coarser scales: comparing a physicist and a philosopher is just dumb, not because they *couldn't* be compared but because they haven't. 

That's what degree classes, and GPAs, and whatever amalgamated measures exist elsewhere, do. What's the problem? *The problem with giving people meaningless numbers is that they will use them to make meaningful decisions*. 


### Employers

I believe what has driven the increase in the fraction of students getting 2.1s and higher is that a 2.1 is often set as the bar for entry to blue-chip employers' graduate programmes. I believe this because this issue has been raised every time I've been a Part II examiner. Why should our students leave with a life-limiting 2.2 when they could have got a 2.1 from `other Russell Group Uni.`?

A colleague once patiently explained to me that the online applications for these programmes feature a drop-down for your (expected) degree class. If we got rid of the class system, what would our students do when faced with this horrifyingly judgemental UI? *Arrgh! It won't submit!!* 

What can one say about this viewpoint other than the obvious: it shows a total lack of faith in the value of our offering.

## Why does nothing change?

The answer is that the system has **massive amounts of friction**

Examining is done by year group, not individual courses.

IOP 

## Tech

We actually own an exam board.

Manually totalling papers

## Exams are bad

I regularly hear my experimental colleagues disparage high performance on exams. People who do well in their exams are "just good at doing algebra". The same way that people who excel at experimental physics are just good at weighing things, I suppose. I'm guessing that the people who say this did well, but not *that* well in their exams.

I concede that the skill of solving problems at breakneck speed for a couple of hours is not particularly "transferable". What's undeniably true, however, is that performance on exams correlates very well with the quality and, yes, quantity of exam preparation, and being able to prepare yourself for a task is pretty much the most valuable skill there is. Doing your research, challenging your understanding, filling the gaps: all parts of acquiring knowledge or a new skill, which you'll do time and again in your lives. 


The pandemic has accelerated the tendency for assessment to completely dominate over every education-related function of the Department.


## The exam process

Things to do differently

1. *Committees are a bad way to do anything*. 